load("C:/Users/hyunho/Shiny/study/3-1/univ_class/MachineLearning/final_project/homeworkdata.RData")
wordmatrix
we_data
head(we_data)
View(wordmatrix)
load("C:\\Users\\hyunho\\Shiny\\study\\3-1\\univ_class\\MachineLearning\\final_project\\homeworkdata.RData")
we_data=load("C:\\Users\\hyunho\\Shiny\\study\\3-1\\univ_class\\MachineLearning\\final_project\\homeworkdata.RData")
we_data=load("C:\\Users\\hyunho\\Shiny\\study\\3-1\\univ_class\\MachineLearning\\final_project\\homeworkdata.RData")
head(we_data)
wordmatrix,we_data=load("C:\\Users\\hyunho\\Shiny\\study\\3-1\\univ_class\\MachineLearning\\final_project\\homeworkdata.RData")
head(we_data)
wordmatrix=load("C:\\Users\\hyunho\\Shiny\\study\\3-1\\univ_class\\MachineLearning\\final_project\\homeworkdata.RData")[0]
head(wordmatrix)
data=load("C:\\Users\\hyunho\\Shiny\\study\\3-1\\univ_class\\MachineLearning\\final_project\\homeworkdata.RData")[0]
data=load("C:\\Users\\hyunho\\Shiny\\study\\3-1\\univ_class\\MachineLearning\\final_project\\homeworkdata.RData")
wordmatrix=data[0]
load("C:\\Users\\hyunho\\Shiny\\study\\3-1\\univ_class\\MachineLearning\\final_project\\homeworkdata.RData")
head(wordmatrix)
head(we_data)
wordmatrix[0]
wordmatrix[1]
#install.packages('e1071')
library(e1071)
data(iris)
iris.subset = subset(iris, select=c("Sepal.Length", "Sepal.Width", "Species"),
Species %in% c("setosa","virginica"))
plot(x=iris.subset$Sepal.Length, y=iris.subset$Sepal.Width, col=iris.subset$Species, pch=19)
svm.model = svm(Species~., data=iris.subset, kernel='linear', cost=1, scale=F)
ls(svm.model)
points(iris.subset[svm.model$index,c(1,2)],col="blue",cex=2)
points(svm.model$SV, col="blue", cex=3)
w = t(svm.model$coefs) %*% svm.model$SV
b = -svm.model$rho
abline(a=-b/w[1,2], b=-w[1,1]/w[1,2], col="red", lty=5)
w[1,2]
w[1,1]
plot(x=iris.subset$Sepal.Length,y=iris.subset$Sepal.Width,
col=iris.subset$Species, pch=19)
svm.model = svm(Species ~ ., data=iris.subset, type='C-classification',
kernel='linear', cost=10000, scale=FALSE)
points(svm.model$SV, col="blue", cex=3)
w = t(svm.model$coefs) %*% svm.model$SV
b = -svm.model$rho
abline(a=-b/w[1,2], b=-w[1,1]/w[1,2], col="red", lty=5)
model.iris = svm(Species~., iris, scale=T)
plot(model.iris, iris, Petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
model.iris
svm.model = svm(Species ~ ., data=iris.subset, type='C-classification',
kernel='linear', cost=10000, scale=FALSE)
points(svm.model$SV, col="blue", cex=3)
w = t(svm.model$coefs) %*% svm.model$SV
b = -svm.model$rho
abline(a=-b/w[1,2], b=-w[1,1]/w[1,2], col="red", lty=5)
svm.model = svm(Species ~ ., data=iris.subset, type='C-classification',
kernel='linear', cost=10000, scale=FALSE)
points(svm.model$SV, col="blue", cex=3)
w = t(svm.model$coefs) %*% svm.model$SV
#install.packages('e1071')
library(e1071)
data(iris)
iris.subset = subset(iris, select=c("Sepal.Length", "Sepal.Width", "Species"),
Species %in% c("setosa","virginica"))
plot(x=iris.subset$Sepal.Length, y=iris.subset$Sepal.Width, col=iris.subset$Species, pch=19)
svm.model = svm(Species~., data=iris.subset, kernel='linear', cost=1, scale=F)
ls(svm.model)
points(iris.subset[svm.model$index,c(1,2)],col="blue",cex=2)
points(svm.model$SV, col="blue", cex=3)
w = t(svm.model$coefs) %*% svm.model$SV
b = -svm.model$rho
abline(a=-b/w[1,2], b=-w[1,1]/w[1,2], col="red", lty=5)
w[1,2]
w[1,1]
plot(x=iris.subset$Sepal.Length,y=iris.subset$Sepal.Width,
col=iris.subset$Species, pch=19)
svm.model = svm(Species ~ ., data=iris.subset, type='C-classification',
kernel='linear', cost=10000, scale=FALSE)
library(randomForest)
data(iris)
head(iris)
typeof(iris)
iris[0]
iris[1]
typeof(wordmatrix)
MB <- randomForest(cat ~., data=wordmatrix, ntree=100, mtry=5, importance=T, na.action=na.omit)
MB <- randomForest(cat ~., data=wordmatrix, ntree=10, mtry=5, importance=T, na.action=na.omit)
head(cat)
head(wordmatrix$cat)
MB <- randomForest(wordmatrix$cat ~., data=wordmatrix, ntree=10, mtry=5, importance=T, na.action=na.omit)
dropna(wordmatrix)
is.na(wordmatrix)
wordmatrix %>% filter(!is.na(wordmatrix))
library(dplyr)
wordmatrix %>% filter(!is.na(wordmatrix))
library(dplyr)
wordmatrix %>% filter(!is.na(wordmatrix))
wordmatrix=na.omit(wordmatrix)
is.na(wordmatrix)
wordmatrix=na.omit(wordmatrix)
na.omit(wordmatrix)
is.na(wordmatrix)
head(wordmatrix
head(wordmatrix)
head(wordmatrix)
load("C:\\Users\\hyunho\\Shiny\\study\\3-1\\univ_class\\MachineLearning\\final_project\\homeworkdata.RData")
head(wordmatrix)
typeof(wordmatrix)
wordmatrix[0]
wordmatrix[1]
wordmatrix[2]
wordmatrix[3]
library(randomForest)
MB <- randomForest(wordmatrix$cat ~., data=wordmatrix, ntree=10, mtry=5, importance=T, na.action=na.omit)
data(iris)
wordmatrix=data_frame(wordmatrix)
wordmatrix=data.frame(wordmatrix)
MB <- randomForest(wordmatrix$cat ~., data=wordmatrix, ntree=10, mtry=5, importance=T, na.action=na.omit)
library(randomForest)
MB <- randomForest(wordmatrix$cat ~., data=wordmatrix, ntree=10, mtry=5, importance=T, na.action=na.omit)
wordmatrix<-data.frame(wordmatrix)
MB <- randomForest(wordmatrix$cat ~., data=wordmatrix, ntree=10, mtry=5, importance=T, na.action=na.omit)
MB <- randomForest(wordmatrix$cat ~., data=word, ntree=10, mtry=5, importance=T, na.action=na.omit)
word <- data.frame(wordmatrix)
MB <- randomForest(wordmatrix$cat ~., data=word, ntree=10, mtry=5, importance=T, na.action=na.omit)
MB <- randomForest(word$cat ~., data=word, ntree=10, mtry=5, importance=T, na.action=na.omit)
head(word)
word[0]
word[1]
word[0][0]
word[1][1]
word <- data.frame(wordmatrix)
library(randomForest)
MB <- randomForest(cat ~., data=word, ntree=10, mtry=5, importance=T, na.action=na.omit)
load("C:\\Users\\hyunho\\Shiny\\study\\3-1\\univ_class\\MachineLearning\\final_project\\homeworkdata.RData")
head(wordmatrix)
head(we_data)
word <- data.frame(wordmatrix)
typeof(word)
head(word)
library(randomForest)
MB <- randomForest(cat ~., data=word, ntree=10, mtry=5, importance=T, na.action=na.omit)
print(MB)
data(iris)
MB <- randomForest(Species ~., data=iris, importance = T, proximity = T)
print(MB)
names(MB)
# 변수마다 중요한 속성값이 다름
round(importance(MB), 2)
MB1 <- data.frame(Sepal.Length = 6.1, Sepal.Width = 3.9, Petal.Length = 1.5, Petal.Width = 0.5)
predictMB1 <- predict(MB, newdata=MB1, type="class")
predictMB1
# 변수를 제거했을때 정확도가 떨어지는것을 볼 수 있음
varImpPlot(MB)
library(MASS)
# factor화 하면 원소마다 갯수 파악 가능
Boston$chas = factor(Boston$chas)
Boston$rad = factor(Boston$rad)
summary(Boston)
rf.Boston <- randomForest(medv ~., data=Boston, ntree=100, mtry=5, importance=T, na.action=na.omit)
summary(rf.Boston)
Boston$medv.hat = predict(rf.Boston, newdata=Boston)
mean((Boston$medv - Boston$medv.hat)^2)
plot(Boston$medv, Boston$medv.hat, xlab="Observed", ylab="Fitted")
for_abline <- lm(Boston$medv ~ Boston$medv.hat, data=Boston)
abline(for_abline, col="blue")
german = read.table("C:\\Users\\hyunho\\Shiny\\study\\3-1\\univ_class\\MachineLearning\\6\\germandata.txt", header = T)
german$numbcredits = factor(german$numcredits)
german$residence = factor(german$residence)
german$residpeople = factor(german$residpeople)
i = sample(1:nrow(german), round(nrow(german)*0.7))
german.train = german[i,]
german.test = german[-i,]
rf.german <- randomForest(y ~. , data = german.train, ntree=100, mtry=5, importance=T, na.action=na.omit)
summary(rf.german)
importance(rf.german, type=1)
pred.rf.german <- predict(rf.german, newdata=german.test)
tab=table(german.test$y, pred.rf.german, dnn=c("Actual", "Predicted"))
print(tab)
error_rate = 1-sum(diag(tab)/sum(tab))
german
typeof(german)
print(MB)
MB <- randomForest(cat ~., data=word, ntree=10, mtry=5, importance=T, na.action=na.omit)
print(MB)
i = sample(1:nrow(word), round(nrow(word)*0.7))
word.train = word[i,]
word.test = word[-i,]
y
rf.word <- randomForest(y ~. , data = word.train, ntree=10, mtry=5, importance=T, na.action=na.omit)
rf.word <- randomForest(cat ~. , data = word.train, ntree=10, mtry=5, importance=T, na.action=na.omit)
summary(rf.word)
pred.rf.word <- predict(rf.word, newdata=word.test)
tab=table(word.test$cat, pred.rf.word, dnn=c("Actual", "Predicted"))
tab
error_rate
error_rate = 1-sum(diag(tab)/sum(tab))
error_rate
load("C:\\Users\\hyunho\\Shiny\\study\\3-1\\univ_class\\MachineLearning\\final_project\\homeworkdata.RData")
word <- data.frame(wordmatrix)
library(randomForest)
i = sample(1:nrow(word), round(nrow(word)*0.7))
word.train = word[i,]
word.test = word[-i,]
rf.word <- randomForest(cat ~. , data = word.train, ntree=50, mtry=5, importance=T, na.action=na.omit)
pred.rf.word <- predict(rf.word, newdata=word.test)
tab=table(word.test$cat, pred.rf.word, dnn=c("Actual", "Predicted"))
tab
error_rate = 1-sum(diag(tab)/sum(tab))
error_rate
MB
word.train
rf.word
load("C:\\Users\\hyunho\\Shiny\\study\\3-1\\univ_class\\MachineLearning\\final_project\\homeworkdata.RData")
word <- data.frame(wordmatrix)
library(randomForest)
MB <- randomForest(cat ~., data=word, ntree=100, mtry=5, importance=T, na.action=na.omit)
print(MB)
MB <- randomForest(cat ~., data=word, importance=T, na.action=na.omit)
print(MB)
MB <- randomForest(cat ~., data=word,ntree=100, mtry=87, importance=T, na.action=na.omit)
print(MB)
MB <- randomForest(cat ~., data=word,ntree=100, mtry=50, importance=T, na.action=na.omit)
print(MB)
MB <- randomForest(cat ~., data=word,ntree=100, mtry=70, importance=T, na.action=na.omit)
print(MB)
MB <- randomForest(cat ~., data=word,ntree=100, mtry=30, importance=T, na.action=na.omit)
print(MB)
MB <- randomForest(cat ~., data=word,ntree=50, mtry=87, importance=T, na.action=na.omit)
print(MB)
MB <- randomForest(cat ~., data=word,ntree=450, mtry=87, importance=T, na.action=na.omit)
print(MB)
MB <- randomForest(cat ~., data=word,importance=T, na.action=na.omit)
print(MB)
i = sample(1:nrow(word), round(nrow(word)*0.7))
word.train = word[i,]
varImpPlot(MB)
word[2]
nrow(word)
ncol(word)
